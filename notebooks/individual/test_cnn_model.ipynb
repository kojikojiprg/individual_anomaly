{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8107d1d-435c-439e-aa32-45a500cade45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid6/home/yokoyama/research\n"
     ]
    }
   ],
   "source": [
    "%cd /home/yokoyama/research\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from modules.utils.video import Capture, Writer\n",
    "from modules.pose import PoseDataHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23efcc3f-7bba-4f42-9834-968691ac62cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from submodules.i3d.pytorch_i3d import InceptionI3d\n",
    "from torchvision.ops import RoIAlign\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78690118",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_num = 1\n",
    "cap = Capture(f\"/raid6/home/yokoyama/datasets/dataset01/train/{video_num:02d}.mp4\")\n",
    "pose_data = PoseDataHandler.load(f\"data/dataset01/train/{video_num:02d}\", [\"bbox\"])\n",
    "flows_raw = np.load(\"data/dataset01/train/01/bin/flow.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10e64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, caps: List[Capture], flows_lst: List[NDArray], pose_data: List[Dict[str, Any]] , seq_len: int, resize_ratio: float):\n",
    "        self._default_float_dtype = torch.get_default_dtype()\n",
    "        self._seq_len = seq_len\n",
    "        self._resize_ratio = resize_ratio\n",
    "        self._frames = []\n",
    "        self._flows = []\n",
    "        self._bboxs = []\n",
    "        self._max_bboxs_num = 0\n",
    "        self._create_dataset(caps, flows_lst, pose_data)\n",
    "\n",
    "        self._frames = self._transform_imgs(self._frames)\n",
    "        self._flows = self._transform_imgs(self._flows)\n",
    "\n",
    "    def _create_dataset(self, caps: List[Capture], flows_lst: List[NDArray], pose_datas: List[List[Dict[str, Any]]]):\n",
    "        for cap, flows, pose_data in zip(tqdm(caps, ncols=100), flows_lst, pose_datas):\n",
    "            self._load_frames(cap)\n",
    "            self._resize_flows(flows)\n",
    "            self._load_bbox(pose_data, cap.frame_count)\n",
    "\n",
    "        # calc max number of bboxs in each frame\n",
    "        for bboxs in self._bboxs:\n",
    "            if len(bboxs) > self._max_bboxs_num:\n",
    "                self._max_bboxs_num = len(bboxs)\n",
    "\n",
    "    def _load_frames(self, cap):\n",
    "        frames = []\n",
    "        for _ in tqdm(range(cap.frame_count), ncols=100, leave=False):\n",
    "        # for _ in tqdm(range(100), ncols=100):\n",
    "            frame = cap.read()[1]\n",
    "            frame = cv2.resize(frame, None, fx=self._resize_ratio, fy=self._resize_ratio)\n",
    "            frames.append(frame)\n",
    "        self._frames += frames\n",
    "\n",
    "    def _resize_flows(self, flows):\n",
    "        flows_resized = []\n",
    "        for flow in tqdm(flows, ncols=100, leave=False):\n",
    "            flows_resized.append(cv2.resize(flow, None, fx=self._resize_ratio, fy=self._resize_ratio))\n",
    "        self._flows += flows_resized\n",
    "\n",
    "    def _load_bbox(self, pose_data: List[Dict[str, Any]], frame_count: int):\n",
    "        for frame_num in tqdm(range(1, frame_count + 1), ncols=100, leave=False):\n",
    "        # for frame_num in tqdm(range(1, 100 + 1), ncols=100, leave=False):\n",
    "            bboxs = [\n",
    "                np.array(data[\"bbox\"]) * self._resize_ratio for data in pose_data\n",
    "                if data[\"frame\"] == frame_num\n",
    "            ]\n",
    "            bboxs = np.array(bboxs)\n",
    "            self._bboxs.append(bboxs)\n",
    "\n",
    "    def _transform_imgs(self, imgs):\n",
    "        # imgs = np.array(imgs)\n",
    "        imgs = torch.tensor(np.array(imgs).transpose((0, 3, 1, 2)), dtype=self._default_float_dtype).contiguous()\n",
    "        # imgs = imgs.to(dtype=self._default_float_dtype)\n",
    "        if isinstance(imgs, torch.ByteTensor):\n",
    "            return (imgs /255.) * 2 - 1\n",
    "        else:\n",
    "            return imgs\n",
    "\n",
    "    @property\n",
    "    def n_samples(self):\n",
    "        return len(self) * self._max_bboxs_num\n",
    "\n",
    "    @property\n",
    "    def n_samples_batch(self):\n",
    "        return self._max_bboxs_num\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._frames) - self._seq_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames_seq = self._frames[idx:idx + self._seq_len].transpose(1, 0)\n",
    "        flows_seq = self._flows[idx:idx + self._seq_len].transpose(1, 0)\n",
    "        bboxs = self._bboxs[idx + (self._seq_len) // 2 + 1]\n",
    "        # append dmy bboxs\n",
    "        if len(bboxs) < self._max_bboxs_num:\n",
    "            diff_num = self._max_bboxs_num - len(bboxs)\n",
    "            dmy_bboxs = [np.full((4,), np.nan) for _ in range(diff_num)]\n",
    "            bboxs = np.append(bboxs, dmy_bboxs, axis=0)\n",
    "        bboxs = torch.Tensor(bboxs)\n",
    "        return frames_seq, flows_seq, bboxs, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed08a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:27<00:00, 27.91s/it]\n"
     ]
    }
   ],
   "source": [
    "seq_len = 30\n",
    "resize_ratio = 0.5\n",
    "device = \"cuda:9\"\n",
    "batch_size = 64\n",
    "dataset = Dataset([cap], [flows_raw], [pose_data], seq_len, resize_ratio)\n",
    "del flows_raw, pose_data, cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362db07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b35fc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576f7085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 10, 470, 640])\n",
      "torch.Size([64, 2, 10, 470, 640])\n",
      "torch.Size([64, 7, 4])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for data in batch:\n",
    "    print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a883888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._i3d_frame = InceptionI3d(in_channels=3, final_endpoint=\"Mixed_3c\")\n",
    "        self._i3d_frame.build()\n",
    "        self._i3d_flow = InceptionI3d(in_channels=2, final_endpoint=\"Mixed_3c\")\n",
    "        self._i3d_flow.build()\n",
    "        self._roi_align = RoIAlign(5, 0.125, 1, aligned=True)\n",
    "\n",
    "    def forward(self, frames, flows, bboxs):\n",
    "        # forward i3d\n",
    "        for end_point in self._i3d_frame.VALID_ENDPOINTS:\n",
    "            if end_point in self._i3d_frame.end_points:\n",
    "                frames = self._i3d_frame._modules[end_point](frames)\n",
    "                flows = self._i3d_flow._modules[end_point](flows)\n",
    "        f = frames + flows\n",
    "\n",
    "        # format bbox\n",
    "        h, w = frames.shape[3:5]\n",
    "        fy, fx = f.shape[3:5]\n",
    "        b = bboxs.shape[0]\n",
    "        bboxs = bboxs.view(-1, 2, 2)\n",
    "        bboxs *= torch.Tensor((fx / w, fy / h))\n",
    "        bboxs = bboxs.view(b, -1, 4)\n",
    "        bboxs = self._convert_bboxes_to_roi_format(bboxs)\n",
    "        bboxs = bboxs.to(torch.float32)\n",
    "\n",
    "        # roi align\n",
    "        return self._roi_align(f, bboxs)\n",
    "\n",
    "    def _convert_bboxes_to_roi_format(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        concat_boxes = torch.cat([b for b in boxes], dim=0)\n",
    "        temp = []\n",
    "        for i, b in enumerate(boxes):\n",
    "            temp.append(torch.full_like(b[:, :1], i))\n",
    "        ids = torch.cat(temp, dim=0)\n",
    "        rois = torch.cat([ids, concat_boxes], dim=1)\n",
    "        return rois\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ngf=64, nc=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(480, ngf * 8, 4, 3, (2, 0), bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            # state size. ``(ngf*8) x 12 x 16``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            # state size. ``(ngf*4) x 24 x 32``\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            # state size. ``(ngf*2) x 48 x 64``\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            # state size. ``(ngf) x 96 x 128``\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # state size. ``(nc) x 192 x 256``\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        n = z.shape[0]\n",
    "        out = self.net(z)\n",
    "        out = out.view(n, 5, 192, 256)\n",
    "        return out[:, :3,], out[:, 3:]  # frame, flow\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._encoder = Encoder()\n",
    "        self._decoder = Decoder()\n",
    "\n",
    "    @property\n",
    "    def E(self):\n",
    "        return self._encoder\n",
    "\n",
    "    @property\n",
    "    def D(self):\n",
    "        return self._decoder\n",
    "\n",
    "    def forward(self, frames, flows, bboxs):\n",
    "        z = self._encoder(frames, flows, bboxs)\n",
    "        frames_d, flows_d = self._decoder(z)\n",
    "\n",
    "        # adjust shapes\n",
    "        b, n = bboxs.shape[:2]\n",
    "        c, sy, sx = z.shape[1:]\n",
    "        z = z.view(b, n, c, sy, sx)\n",
    "        frames_d = frames_d.view(b, n, 3, 192, 256)\n",
    "        flows_d = flows_d.view(b, n, 2, 192, 256)\n",
    "\n",
    "        return z, frames_d, flows_d\n",
    "\n",
    "\n",
    "class ClusteringModule(nn.Module):\n",
    "    def __init__(self, n_clusters, n_samples):\n",
    "        super().__init__()\n",
    "        self._n_clusters = n_clusters\n",
    "        self._n_samples = n_samples\n",
    "        self._t_alpha = 1\n",
    "        self._dz = 20\n",
    "        self._centroids = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn((self._dz), dtype=torch.float32)) for _ in range(n_clusters)]\n",
    "        )\n",
    "        self._target_distribution = None\n",
    "        self.clear_target_disribution()\n",
    "\n",
    "        self._emb = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(480 * 5 * 5, self._dz),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def centroids(self):\n",
    "        return self._centroids\n",
    "\n",
    "    @property\n",
    "    def target_distribution(self):\n",
    "        return self._target_distribution\n",
    "\n",
    "    def forward(self, z):\n",
    "        b, sn = z.shape[:2]\n",
    "        z = z.view(b * sn, -1)\n",
    "        z = self._emb(z)\n",
    "        s = self._student_t(z)\n",
    "        s = s.view(b, -1, self._n_clusters)\n",
    "        c = s.argmax(dim=2)\n",
    "        return s, c\n",
    "\n",
    "    def _student_t(self, z):\n",
    "        sn = z.shape[0]\n",
    "        norm = torch.full((sn, self._n_clusters), torch.nan, dtype=torch.float32)\n",
    "        for j in range(self._n_clusters):\n",
    "            norm[:, j] = torch.linalg.vector_norm(z - self._centroids[j], dim=1)\n",
    "\n",
    "        s = torch.full((sn, self._n_clusters), torch.nan, dtype=torch.float32)\n",
    "        for i in range(sn):\n",
    "            s[i] = ((1 + norm[i]) / self._t_alpha)**-((self._t_alpha + 1) / 2)\n",
    "        s = (s.T / s.sum(dim=1)).T\n",
    "\n",
    "        return s\n",
    "\n",
    "    def clear_target_disribution(self):\n",
    "        self._target_distribution = torch.full((self._n_samples, self._n_clusters), torch.nan)\n",
    "\n",
    "    def update_target_distribution(self, s, batch_idxs):\n",
    "        sample_nums = s.shape[1]\n",
    "        s = s.view(-1, self._n_clusters)\n",
    "        s_sums = s.nan_to_num(0).sum(dim=0)  # Sigma_i s_ij (n_clusters,)\n",
    "\n",
    "        for i, batch_idx in enumerate(batch_idxs):\n",
    "            for sn in range(sample_nums):\n",
    "                ti = batch_idx * self._n_clusters + sn  # target idx\n",
    "                si = i * self._n_clusters + sn  # soft idx\n",
    "                for j in range(self._n_clusters):\n",
    "                    sij = s[si, j]\n",
    "                    self._target_distribution[ti, j] = sij**2 / s_sums[j]\n",
    "                self._target_distribution[ti, j] /= self._target_distribution[ti, j].sum(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280369bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "ae = Autoencoder().to(device)\n",
    "cm = ClusteringModule(n_clusters, dataset.n_samples).to(device)\n",
    "\n",
    "loss_r = nn.MSELoss().to(device)\n",
    "loss_c = nn.KLDivLoss(reduction=\"sum\").to(device)\n",
    "\n",
    "optim_e = torch.optim.Adam(ae.E.parameters(), 0.0001)\n",
    "optim_d = torch.optim.Adam(ae.D.parameters(), 0.0001)\n",
    "optim_c = torch.optim.Adam(cm.parameters(), 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2625b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/100 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.74 GiB (GPU 9; 10.75 GiB total capacity; 6.91 GiB already allocated; 2.93 GiB free; 6.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m optim_d\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m optim_c\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m z, frames_out, flows_out \u001b[39m=\u001b[39m ae(frames_batch, flows_batch, bboxs_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m s, c \u001b[39m=\u001b[39m cm(z)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch_idx \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch_idxs):\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, frames, flows, bboxs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder(frames, flows, bboxs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     frames_d, flows_d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder(z)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m     \u001b[39m# adjust shapes\u001b[39;00m\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m end_point \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i3d_frame\u001b[39m.\u001b[39mVALID_ENDPOINTS:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m end_point \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i3d_frame\u001b[39m.\u001b[39mend_points:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_i3d_frame\u001b[39m.\u001b[39;49m_modules[end_point](frames)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         flows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i3d_flow\u001b[39m.\u001b[39m_modules[end_point](flows)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m f \u001b[39m=\u001b[39m frames \u001b[39m+\u001b[39m flows\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/submodules/i3d/pytorch_i3d.py:115\u001b[0m, in \u001b[0;36mUnit3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(x, pad)\n\u001b[1;32m    113\u001b[0m \u001b[39m#print x.size()        \u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3d(x)\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_batch_norm:\n\u001b[1;32m    117\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:592\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:587\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    576\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[1;32m    577\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    578\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    586\u001b[0m     )\n\u001b[0;32m--> 587\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[1;32m    588\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    589\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.74 GiB (GPU 9; 10.75 GiB total capacity; 6.91 GiB already allocated; 2.93 GiB free; 6.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "update_interval = 10\n",
    "\n",
    "history = {\n",
    "    \"lr\": [],\n",
    "    \"lc\": [],\n",
    "    \"le\": [],\n",
    "    \"c\": [],\n",
    "}\n",
    "for epoch in tqdm(range(epoch_num), ncols=100):\n",
    "    ae.train()\n",
    "    cm.train()\n",
    "    c_epoch = torch.full((dataset.n_samples,), torch.nan).to(device)\n",
    "    for batch_idx, (frames_batch, flows_batch, bboxs_batch, batch_idxs) in enumerate(tqdm(dataloader, ncols=100, leave=False)):\n",
    "        frames_batch, flows_batch = frames_batch.to(device), flows_batch.to(device)\n",
    "        bboxs_batch, batch_idxs = bboxs_batch.to(device), batch_idxs.to(device)\n",
    "\n",
    "        optim_e.zero_grad()\n",
    "        optim_d.zero_grad()\n",
    "        optim_c.zero_grad()\n",
    "\n",
    "        z, frames_out, flows_out = ae(frames_batch, flows_batch, bboxs_batch)\n",
    "        s, c = cm(z)\n",
    "\n",
    "        for i, batch_idx in enumerate(batch_idxs):\n",
    "            for j in range(dataset.n_samples_batch):\n",
    "                idx = batch_idx * dataset.n_samples_batch + j\n",
    "                c_epoch[idx] = c[i, j]\n",
    "\n",
    "        lr_total = 0\n",
    "        for i in range(batch_size):\n",
    "            for j in range(dataset.n_samples_batch):\n",
    "                bx = bboxs_batch[i, j]\n",
    "                x1, y1, x2, y2 = bx\n",
    "                frame_bbox = frames_batch[i, seq_len // 2 + 1, y1:y2, x1:x2]\n",
    "                lr_total += loss_r(frames_out[i, j], frame_bbox)\n",
    "        lr_total.backward()\n",
    "        optim_d.step()\n",
    "\n",
    "        lc_total = 0\n",
    "        for i, batch_idx in enumerate(batch_idxs):\n",
    "            idx = batch_idx * dataset.n_samples_batch\n",
    "            tmp_target = cm.target_distribution[idx:idx + dataset.n_samples_batch]\n",
    "            tmp_target = torch.nan_to_num(tmp_target, 0)\n",
    "            s_tmp = torch.nan_to_num(s[i], 0)\n",
    "            lc_total += loss_c(s_tmp.log(), tmp_target)\n",
    "        lc_total.backward()\n",
    "\n",
    "        optim_c.step()\n",
    "\n",
    "        le = lr_total + lc_total\n",
    "        le.backward()\n",
    "        optim_e.step()\n",
    "\n",
    "        history[\"lr\"].append(lr_total.cpu())\n",
    "        history[\"lc\"].append(lc_total.cpu())\n",
    "        history[\"le\"].append(le.cpu())\n",
    "\n",
    "    if epoch % update_interval == 0:\n",
    "        cm.update_target_distribution()\n",
    "        history[\"c\"].append(c)\n",
    "    tqdm.write(f\"epoch:{epoch}, lr:{lr_total:04f}, lc:{lc_total:04f}, le:{le:04f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889bf34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
