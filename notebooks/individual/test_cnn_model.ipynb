{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8107d1d-435c-439e-aa32-45a500cade45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid6/home/yokoyama/research\n"
     ]
    }
   ],
   "source": [
    "%cd /home/yokoyama/research\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from modules.utils.video import Capture, Writer\n",
    "from modules.pose import PoseDataHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23efcc3f-7bba-4f42-9834-968691ac62cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from submodules.i3d.pytorch_i3d import InceptionI3d\n",
    "from torchvision.ops import RoIAlign\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78690118",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_num = 1\n",
    "cap = Capture(f\"/raid6/home/yokoyama/datasets/dataset01/train/{video_num:02d}.mp4\")\n",
    "pose_data = PoseDataHandler.load(f\"data/dataset01/train/{video_num:02d}\", [\"bbox\"])\n",
    "flows_raw = np.load(\"data/dataset01/train/01/bin/flow.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10e64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, caps: List[Capture], flows_lst: List[NDArray], pose_data: List[Dict[str, Any]] , seq_len: int, resize_ratio: float):\n",
    "        self._default_float_dtype = torch.get_default_dtype()\n",
    "        self._seq_len = seq_len\n",
    "        self._resize_ratio = resize_ratio\n",
    "        self._frames = []\n",
    "        self._flows = []\n",
    "        self._bboxs = []\n",
    "        self._max_bboxs_num = 0\n",
    "        self._create_dataset(caps, flows_lst, pose_data)\n",
    "\n",
    "        self._frames = self._transform_imgs(self._frames)\n",
    "        self._flows = self._transform_imgs(self._flows)\n",
    "\n",
    "    def _create_dataset(self, caps: List[Capture], flows_lst: List[NDArray], pose_datas: List[List[Dict[str, Any]]]):\n",
    "        for cap, flows, pose_data in zip(tqdm(caps, ncols=100), flows_lst, pose_datas):\n",
    "            self._load_frames(cap)\n",
    "            self._resize_flows(flows)\n",
    "            self._load_bbox(pose_data, cap.frame_count)\n",
    "\n",
    "        # calc max number of bboxs in each frame\n",
    "        for bboxs in self._bboxs:\n",
    "            if len(bboxs) > self._max_bboxs_num:\n",
    "                self._max_bboxs_num = len(bboxs)\n",
    "\n",
    "    def _load_frames(self, cap):\n",
    "        frames = []\n",
    "        for _ in tqdm(range(cap.frame_count), ncols=100, leave=False):\n",
    "        # for _ in tqdm(range(100), ncols=100):\n",
    "            frame = cap.read()[1]\n",
    "            frame = cv2.resize(frame, None, fx=self._resize_ratio, fy=self._resize_ratio)\n",
    "            frames.append(frame)\n",
    "        self._frames += frames\n",
    "\n",
    "    def _resize_flows(self, flows):\n",
    "        flows_resized = []\n",
    "        for flow in tqdm(flows, ncols=100, leave=False):\n",
    "            flows_resized.append(cv2.resize(flow, None, fx=self._resize_ratio, fy=self._resize_ratio))\n",
    "        self._flows += flows_resized\n",
    "\n",
    "    def _load_bbox(self, pose_data: List[Dict[str, Any]], frame_count: int):\n",
    "        for frame_num in tqdm(range(1, frame_count + 1), ncols=100, leave=False):\n",
    "        # for frame_num in tqdm(range(1, 100 + 1), ncols=100, leave=False):\n",
    "            bboxs = [\n",
    "                np.array(data[\"bbox\"]) * self._resize_ratio for data in pose_data\n",
    "                if data[\"frame\"] == frame_num\n",
    "            ]\n",
    "            bboxs = np.array(bboxs)\n",
    "            self._bboxs.append(bboxs)\n",
    "\n",
    "    def _transform_imgs(self, imgs):\n",
    "        # imgs = np.array(imgs)\n",
    "        imgs = torch.tensor(np.array(imgs).transpose((0, 3, 1, 2)), dtype=self._default_float_dtype).contiguous()\n",
    "        # imgs = imgs.to(dtype=self._default_float_dtype)\n",
    "        if isinstance(imgs, torch.ByteTensor):\n",
    "            return (imgs /255.) * 2 - 1\n",
    "        else:\n",
    "            return imgs\n",
    "\n",
    "    @property\n",
    "    def n_samples(self):\n",
    "        return len(self) * self._max_bboxs_num\n",
    "\n",
    "    @property\n",
    "    def n_samples_batch(self):\n",
    "        return self._max_bboxs_num\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._frames) - self._seq_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        st = time.time()\n",
    "        frames_seq = self._frames[idx:idx + self._seq_len].transpose(1, 0)\n",
    "        et = time.time()\n",
    "        print(et - st)\n",
    "        st = time.time()\n",
    "        flows_seq = self._flows[idx:idx + self._seq_len].transpose(1, 0)\n",
    "        et = time.time()\n",
    "        print(et - st)\n",
    "        st = time.time()\n",
    "        bboxs = self._bboxs[idx + (self._seq_len) // 2 + 1]\n",
    "        # append dmy bboxs\n",
    "        if len(bboxs) < self._max_bboxs_num:\n",
    "            diff_num = self._max_bboxs_num - len(bboxs)\n",
    "            dmy_bboxs = [np.full((4,), np.nan) for _ in range(diff_num)]\n",
    "            bboxs = np.append(bboxs, dmy_bboxs, axis=0)\n",
    "        bboxs = torch.Tensor(bboxs)\n",
    "        et = time.time()\n",
    "        print(et - st)\n",
    "        return frames_seq, flows_seq, bboxs, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed08a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:29<00:00, 29.59s/it]\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "resize_ratio = 0.5\n",
    "device = \"cuda:9\"\n",
    "batch_size = 64\n",
    "dataset = Dataset([cap], [flows_raw], [pose_data], seq_len, resize_ratio)\n",
    "del flows_raw, pose_data, cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362db07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b35fc708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001494884490966797\n",
      "0.00021076202392578125\n",
      "0.00016760826110839844\n",
      "1.1682510375976562e-05\n",
      "6.198883056640625e-06\n",
      "4.6253204345703125e-05\n",
      "8.821487426757812e-06\n",
      "5.9604644775390625e-06\n",
      "4.100799560546875e-05\n",
      "9.775161743164062e-06\n",
      "6.4373016357421875e-06\n",
      "3.170967102050781e-05\n",
      "9.298324584960938e-06\n",
      "6.4373016357421875e-06\n",
      "3.552436828613281e-05\n",
      "7.867813110351562e-06\n",
      "5.7220458984375e-06\n",
      "3.600120544433594e-05\n",
      "7.867813110351562e-06\n",
      "7.152557373046875e-06\n",
      "3.552436828613281e-05\n",
      "8.344650268554688e-06\n",
      "5.7220458984375e-06\n",
      "3.457069396972656e-05\n",
      "7.867813110351562e-06\n",
      "5.9604644775390625e-06\n",
      "3.7670135498046875e-05\n",
      "8.344650268554688e-06\n",
      "5.9604644775390625e-06\n",
      "3.7670135498046875e-05\n",
      "8.58306884765625e-06\n",
      "6.4373016357421875e-06\n",
      "3.552436828613281e-05\n",
      "8.106231689453125e-06\n",
      "5.7220458984375e-06\n",
      "3.2901763916015625e-05\n",
      "7.867813110351562e-06\n",
      "5.7220458984375e-06\n",
      "3.5762786865234375e-05\n",
      "7.62939453125e-06\n",
      "5.9604644775390625e-06\n",
      "3.409385681152344e-05\n",
      "7.867813110351562e-06\n",
      "5.9604644775390625e-06\n",
      "3.123283386230469e-05\n",
      "8.106231689453125e-06\n",
      "5.9604644775390625e-06\n",
      "3.147125244140625e-05\n",
      "8.106231689453125e-06\n",
      "6.198883056640625e-06\n",
      "3.123283386230469e-05\n",
      "7.62939453125e-06\n",
      "5.7220458984375e-06\n",
      "3.218650817871094e-05\n",
      "8.106231689453125e-06\n",
      "5.4836273193359375e-06\n",
      "3.1948089599609375e-05\n",
      "8.106231689453125e-06\n",
      "5.7220458984375e-06\n",
      "3.1948089599609375e-05\n",
      "7.62939453125e-06\n",
      "5.7220458984375e-06\n",
      "3.24249267578125e-05\n",
      "7.62939453125e-06\n",
      "5.7220458984375e-06\n",
      "2.9087066650390625e-05\n",
      "8.344650268554688e-06\n",
      "5.9604644775390625e-06\n",
      "3.4332275390625e-05\n",
      "8.106231689453125e-06\n",
      "5.7220458984375e-06\n",
      "4.172325134277344e-05\n",
      "7.62939453125e-06\n",
      "5.4836273193359375e-06\n",
      "3.790855407714844e-05\n",
      "8.106231689453125e-06\n",
      "5.9604644775390625e-06\n",
      "3.147125244140625e-05\n",
      "7.62939453125e-06\n",
      "6.4373016357421875e-06\n",
      "3.2901763916015625e-05\n",
      "7.867813110351562e-06\n",
      "5.4836273193359375e-06\n",
      "3.457069396972656e-05\n",
      "7.867813110351562e-06\n",
      "5.245208740234375e-06\n",
      "3.504753112792969e-05\n",
      "7.867813110351562e-06\n",
      "5.7220458984375e-06\n",
      "3.743171691894531e-05\n",
      "7.62939453125e-06\n",
      "5.4836273193359375e-06\n",
      "3.4332275390625e-05\n",
      "7.867813110351562e-06\n",
      "5.9604644775390625e-06\n",
      "3.409385681152344e-05\n",
      "7.867813110351562e-06\n",
      "5.7220458984375e-06\n",
      "3.0994415283203125e-05\n",
      "1.1205673217773438e-05\n",
      "5.9604644775390625e-06\n",
      "3.5762786865234375e-05\n",
      "7.62939453125e-06\n",
      "5.7220458984375e-06\n",
      "3.2901763916015625e-05\n",
      "7.62939453125e-06\n",
      "5.245208740234375e-06\n",
      "3.147125244140625e-05\n",
      "7.62939453125e-06\n",
      "5.7220458984375e-06\n",
      "3.7670135498046875e-05\n",
      "7.62939453125e-06\n",
      "5.9604644775390625e-06\n",
      "3.457069396972656e-05\n",
      "7.867813110351562e-06\n",
      "5.4836273193359375e-06\n",
      "3.337860107421875e-05\n",
      "7.867813110351562e-06\n",
      "5.4836273193359375e-06\n",
      "3.62396240234375e-05\n",
      "9.5367431640625e-06\n",
      "6.4373016357421875e-06\n",
      "3.457069396972656e-05\n",
      "7.867813110351562e-06\n",
      "5.7220458984375e-06\n",
      "4.124641418457031e-05\n",
      "8.344650268554688e-06\n",
      "5.7220458984375e-06\n",
      "3.147125244140625e-05\n",
      "7.62939453125e-06\n",
      "5.4836273193359375e-06\n",
      "3.457069396972656e-05\n",
      "7.62939453125e-06\n",
      "5.245208740234375e-06\n",
      "3.0994415283203125e-05\n",
      "8.344650268554688e-06\n",
      "5.9604644775390625e-06\n",
      "2.8133392333984375e-05\n",
      "7.3909759521484375e-06\n",
      "6.198883056640625e-06\n",
      "3.075599670410156e-05\n",
      "7.3909759521484375e-06\n",
      "7.3909759521484375e-06\n",
      "3.409385681152344e-05\n",
      "7.62939453125e-06\n",
      "5.9604644775390625e-06\n",
      "3.4809112548828125e-05\n",
      "7.867813110351562e-06\n",
      "5.7220458984375e-06\n",
      "3.0279159545898438e-05\n",
      "7.152557373046875e-06\n",
      "6.198883056640625e-06\n",
      "3.075599670410156e-05\n",
      "7.62939453125e-06\n",
      "5.245208740234375e-06\n",
      "3.4809112548828125e-05\n",
      "7.62939453125e-06\n",
      "5.7220458984375e-06\n",
      "3.123283386230469e-05\n",
      "7.867813110351562e-06\n",
      "5.245208740234375e-06\n",
      "3.3855438232421875e-05\n",
      "7.62939453125e-06\n",
      "7.62939453125e-06\n",
      "3.719329833984375e-05\n",
      "7.3909759521484375e-06\n",
      "5.9604644775390625e-06\n",
      "2.956390380859375e-05\n",
      "7.3909759521484375e-06\n",
      "5.245208740234375e-06\n",
      "3.337860107421875e-05\n",
      "7.62939453125e-06\n",
      "5.7220458984375e-06\n",
      "3.457069396972656e-05\n",
      "7.3909759521484375e-06\n",
      "5.4836273193359375e-06\n",
      "3.62396240234375e-05\n",
      "7.3909759521484375e-06\n",
      "5.7220458984375e-06\n",
      "3.337860107421875e-05\n",
      "7.867813110351562e-06\n",
      "5.7220458984375e-06\n",
      "3.981590270996094e-05\n",
      "8.344650268554688e-06\n",
      "5.4836273193359375e-06\n",
      "3.24249267578125e-05\n",
      "8.58306884765625e-06\n",
      "5.4836273193359375e-06\n",
      "3.218650817871094e-05\n",
      "7.152557373046875e-06\n",
      "5.7220458984375e-06\n",
      "3.361701965332031e-05\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576f7085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 10, 470, 640])\n",
      "torch.Size([64, 2, 10, 470, 640])\n",
      "torch.Size([64, 7, 4])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for data in batch:\n",
    "    print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a883888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._i3d_frame = InceptionI3d(in_channels=3, final_endpoint=\"Mixed_3c\")\n",
    "        self._i3d_frame.build()\n",
    "        self._i3d_flow = InceptionI3d(in_channels=2, final_endpoint=\"Mixed_3c\")\n",
    "        self._i3d_flow.build()\n",
    "        self._roi_align = RoIAlign(5, 0.125, 1, aligned=True)\n",
    "\n",
    "    def forward(self, frames, flows, bboxs):\n",
    "        # forward i3d\n",
    "        for end_point in self._i3d_frame.VALID_ENDPOINTS:\n",
    "            if end_point in self._i3d_frame.end_points:\n",
    "                frames = self._i3d_frame._modules[end_point](frames)\n",
    "                flows = self._i3d_flow._modules[end_point](flows)\n",
    "        f = frames + flows\n",
    "\n",
    "        # format bbox\n",
    "        h, w = frames.shape[3:5]\n",
    "        fy, fx = f.shape[3:5]\n",
    "        b = bboxs.shape[0]\n",
    "        bboxs = bboxs.view(-1, 2, 2)\n",
    "        bboxs *= torch.Tensor((fx / w, fy / h))\n",
    "        bboxs = bboxs.view(b, -1, 4)\n",
    "        bboxs = self._convert_bboxes_to_roi_format(bboxs)\n",
    "        bboxs = bboxs.to(torch.float32)\n",
    "\n",
    "        # roi align\n",
    "        return self._roi_align(f, bboxs)\n",
    "\n",
    "    def _convert_bboxes_to_roi_format(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        concat_boxes = torch.cat([b for b in boxes], dim=0)\n",
    "        temp = []\n",
    "        for i, b in enumerate(boxes):\n",
    "            temp.append(torch.full_like(b[:, :1], i))\n",
    "        ids = torch.cat(temp, dim=0)\n",
    "        rois = torch.cat([ids, concat_boxes], dim=1)\n",
    "        return rois\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ngf=64, nc=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(480, ngf * 8, 4, 3, (2, 0), bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            # state size. ``(ngf*8) x 12 x 16``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            # state size. ``(ngf*4) x 24 x 32``\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            # state size. ``(ngf*2) x 48 x 64``\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            # state size. ``(ngf) x 96 x 128``\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # state size. ``(nc) x 192 x 256``\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        n = z.shape[0]\n",
    "        out = self.net(z)\n",
    "        out = out.view(n, 5, 192, 256)\n",
    "        return out[:, :3,], out[:, 3:]  # frame, flow\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._encoder = Encoder()\n",
    "        self._decoder = Decoder()\n",
    "\n",
    "    @property\n",
    "    def E(self):\n",
    "        return self._encoder\n",
    "\n",
    "    @property\n",
    "    def D(self):\n",
    "        return self._decoder\n",
    "\n",
    "    def forward(self, frames, flows, bboxs):\n",
    "        z = self._encoder(frames, flows, bboxs)\n",
    "        frames_d, flows_d = self._decoder(z)\n",
    "\n",
    "        # adjust shapes\n",
    "        b, n = bboxs.shape[:2]\n",
    "        c, sy, sx = z.shape[1:]\n",
    "        z = z.view(b, n, c, sy, sx)\n",
    "        frames_d = frames_d.view(b, n, 3, 192, 256)\n",
    "        flows_d = flows_d.view(b, n, 2, 192, 256)\n",
    "\n",
    "        return z, frames_d, flows_d\n",
    "\n",
    "\n",
    "class ClusteringModule(nn.Module):\n",
    "    def __init__(self, n_clusters, n_samples):\n",
    "        super().__init__()\n",
    "        self._n_clusters = n_clusters\n",
    "        self._n_samples = n_samples\n",
    "        self._t_alpha = 1\n",
    "        self._dz = 20\n",
    "        self._centroids = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn((self._dz), dtype=torch.float32)) for _ in range(n_clusters)]\n",
    "        )\n",
    "        self._target_distribution = None\n",
    "        self.clear_target_disribution()\n",
    "\n",
    "        self._emb = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(480 * 5 * 5, self._dz),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def centroids(self):\n",
    "        return self._centroids\n",
    "\n",
    "    @property\n",
    "    def target_distribution(self):\n",
    "        return self._target_distribution\n",
    "\n",
    "    def forward(self, z):\n",
    "        b, sn = z.shape[:2]\n",
    "        z = z.view(b * sn, -1)\n",
    "        z = self._emb(z)\n",
    "        s = self._student_t(z)\n",
    "        s = s.view(b, -1, self._n_clusters)\n",
    "        c = s.argmax(dim=2)\n",
    "        return s, c\n",
    "\n",
    "    def _student_t(self, z):\n",
    "        sn = z.shape[0]\n",
    "        norm = torch.full((sn, self._n_clusters), torch.nan, dtype=torch.float32)\n",
    "        for j in range(self._n_clusters):\n",
    "            norm[:, j] = torch.linalg.vector_norm(z - self._centroids[j], dim=1)\n",
    "\n",
    "        s = torch.full((sn, self._n_clusters), torch.nan, dtype=torch.float32)\n",
    "        for i in range(sn):\n",
    "            s[i] = ((1 + norm[i]) / self._t_alpha)**-((self._t_alpha + 1) / 2)\n",
    "        s = (s.T / s.sum(dim=1)).T\n",
    "\n",
    "        return s\n",
    "\n",
    "    def clear_target_disribution(self):\n",
    "        self._target_distribution = torch.full((self._n_samples, self._n_clusters), torch.nan)\n",
    "\n",
    "    def update_target_distribution(self, s, batch_idxs):\n",
    "        sample_nums = s.shape[1]\n",
    "        s = s.view(-1, self._n_clusters)\n",
    "        s_sums = s.nan_to_num(0).sum(dim=0)  # Sigma_i s_ij (n_clusters,)\n",
    "\n",
    "        for i, batch_idx in enumerate(batch_idxs):\n",
    "            for sn in range(sample_nums):\n",
    "                ti = batch_idx * self._n_clusters + sn  # target idx\n",
    "                si = i * self._n_clusters + sn  # soft idx\n",
    "                for j in range(self._n_clusters):\n",
    "                    sij = s[si, j]\n",
    "                    self._target_distribution[ti, j] = sij**2 / s_sums[j]\n",
    "                self._target_distribution[ti, j] /= self._target_distribution[ti, j].sum(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280369bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "ae = Autoencoder().to(device)\n",
    "cm = ClusteringModule(n_clusters, dataset.n_samples).to(device)\n",
    "\n",
    "loss_r = nn.MSELoss().to(device)\n",
    "loss_c = nn.KLDivLoss(reduction=\"sum\").to(device)\n",
    "\n",
    "optim_e = torch.optim.Adam(ae.E.parameters(), 0.0001)\n",
    "optim_d = torch.optim.Adam(ae.D.parameters(), 0.0001)\n",
    "optim_c = torch.optim.Adam(cm.parameters(), 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2625b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/100 [00:17<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_3996284/737081480.py\", line 98, in __getitem__\n    return frames_seq.to(self._device), flows_seq.to(self._device), bboxs.to(self._device), idx.to(self._device)\n  File \"/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 206, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m cm\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m c_epoch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((dataset\u001b[39m.\u001b[39mn_samples,), torch\u001b[39m.\u001b[39mnan)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (frames_batch, flows_batch, bboxs_batch, batch_idxs) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataloader, ncols\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     frames_batch, flows_batch \u001b[39m=\u001b[39m frames_batch\u001b[39m.\u001b[39mto(device), flows_batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.240.22.200/raid6/home/yokoyama/research/notebooks/individual/test_cnn_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     bboxs_batch, batch_idxs \u001b[39m=\u001b[39m bboxs_batch\u001b[39m.\u001b[39mto(device), batch_idxs\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1250\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1251\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 457\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_3996284/737081480.py\", line 98, in __getitem__\n    return frames_seq.to(self._device), flows_seq.to(self._device), bboxs.to(self._device), idx.to(self._device)\n  File \"/raid6/home/yokoyama/research/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 206, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "update_interval = 10\n",
    "\n",
    "history = {\n",
    "    \"lr\": [],\n",
    "    \"lc\": [],\n",
    "    \"le\": [],\n",
    "    \"c\": [],\n",
    "}\n",
    "for epoch in tqdm(range(epoch_num), ncols=100):\n",
    "    ae.train()\n",
    "    cm.train()\n",
    "    c_epoch = torch.full((dataset.n_samples,), torch.nan).to(device)\n",
    "    for batch_idx, (frames_batch, flows_batch, bboxs_batch, batch_idxs) in enumerate(tqdm(dataloader, ncols=100, leave=False)):\n",
    "        frames_batch, flows_batch = frames_batch.to(device), flows_batch.to(device)\n",
    "        bboxs_batch, batch_idxs = bboxs_batch.to(device), batch_idxs.to(device)\n",
    "\n",
    "        optim_e.zero_grad()\n",
    "        optim_d.zero_grad()\n",
    "        optim_c.zero_grad()\n",
    "\n",
    "        z, frames_out, flows_out = ae(frames_batch, flows_batch, bboxs_batch)\n",
    "        s, c = cm(z)\n",
    "\n",
    "        for i, batch_idx in enumerate(batch_idxs):\n",
    "            for j in range(dataset.n_samples_batch):\n",
    "                idx = batch_idx * dataset.n_samples_batch + j\n",
    "                c_epoch[idx] = c[i, j]\n",
    "\n",
    "        lr_total = 0\n",
    "        for i in range(batch_size):\n",
    "            for j in range(dataset.n_samples_batch):\n",
    "                bx = bboxs_batch[i, j]\n",
    "                x1, y1, x2, y2 = bx\n",
    "                frame_bbox = frames_batch[i, seq_len // 2 + 1, y1:y2, x1:x2]\n",
    "                lr_total += loss_r(frames_out[i, j], frame_bbox)\n",
    "        lr_total.backward()\n",
    "        optim_d.step()\n",
    "\n",
    "        lc_total = 0\n",
    "        for i, batch_idx in enumerate(batch_idxs):\n",
    "            idx = batch_idx * dataset.n_samples_batch\n",
    "            tmp_target = cm.target_distribution[idx:idx + dataset.n_samples_batch]\n",
    "            tmp_target = torch.nan_to_num(tmp_target, 0)\n",
    "            s_tmp = torch.nan_to_num(s[i], 0)\n",
    "            lc_total += loss_c(s_tmp.log(), tmp_target)\n",
    "        lc_total.backward()\n",
    "\n",
    "        optim_c.step()\n",
    "\n",
    "        le = lr_total + lc_total\n",
    "        le.backward()\n",
    "        optim_e.step()\n",
    "\n",
    "        history[\"lr\"].append(lr_total.cpu())\n",
    "        history[\"lc\"].append(lc_total.cpu())\n",
    "        history[\"le\"].append(le.cpu())\n",
    "\n",
    "    if epoch % update_interval == 0:\n",
    "        cm.update_target_distribution()\n",
    "        history[\"c\"].append(c)\n",
    "    tqdm.write(f\"epoch:{epoch}, lr:{lr_total:04f}, lc:{lc_total:04f}, le:{le:04f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889bf34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
